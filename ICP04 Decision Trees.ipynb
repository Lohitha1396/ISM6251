{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a09374",
   "metadata": {},
   "source": [
    "# SETTING UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b37d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900f5f8",
   "metadata": {},
   "source": [
    "# LOAD THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b90a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./airbnb_train_X_price_gte_150.csv') \n",
    "y_train = pd.read_csv('./airbnb_train_y_price_gte_150.csv') \n",
    "X_test = pd.read_csv('./airbnb_test_X_price_gte_150.csv') \n",
    "y_test = pd.read_csv('./airbnb_test_y_price_gte_150.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a7210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ab44f",
   "metadata": {},
   "source": [
    "# SVM POLYNOMIAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c53358e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly_model = SVC(kernel=\"poly\", degree=3, coef0=1, C=10)\n",
    "_ = svm_poly_model.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7183c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.867854</td>\n",
       "      <td>0.855839</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.869323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.867854</td>\n",
       "      <td>0.855839</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.869323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm with polynomial kernel</td>\n",
       "      <td>0.867854</td>\n",
       "      <td>0.855839</td>\n",
       "      <td>0.883239</td>\n",
       "      <td>0.869323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  Accuracy  Precision    Recall        F1\n",
       "0  svm with polynomial kernel  0.867854   0.855839  0.883239  0.869323\n",
       "0  svm with polynomial kernel  0.867854   0.855839  0.883239  0.869323\n",
       "0  svm with polynomial kernel  0.867854   0.855839  0.883239  0.869323"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_preds = svm_poly_model.predict(X_test)\n",
    "c_matrix = confusion_matrix(y_test, model_preds)\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"svm with polynomial kernel\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df325c3",
   "metadata": {},
   "source": [
    "# RANDOM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0503052c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.8549791238076624\n",
      "... with parameters: {'min_samples_split': 16, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0036, 'max_leaf_nodes': 74, 'max_depth': 13, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18137\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "55 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "55 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\18137\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\18137\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\18137\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\18137\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.84271483 0.77844887 0.83766907 0.83997506 0.83987605 0.82488631\n",
      " 0.84276655 0.82488631 0.82488631 0.84173701 0.82488631 0.84412142\n",
      " 0.82488631 0.82488631 0.83026191 0.83585906 0.83253161 0.83834604\n",
      " 0.82488631 0.83585906 0.84740936 0.8360193  0.8278815  0.82488631\n",
      " 0.82763596 0.82488631 0.82970101 0.84422075 0.85497912 0.82488631\n",
      " 0.82908713 0.83304336 0.82497009 0.83847851 0.84793507 0.82627955\n",
      " 0.83507541 0.85063152 0.85020423 0.82488631 0.84173701 0.82488631\n",
      " 0.83143504 0.84123466 0.834675   0.82488631 0.83987605 0.77844887\n",
      " 0.82488631 0.82488631 0.83212638 0.83585906 0.85255081 0.83781623\n",
      " 0.8280068  0.83451668 0.82901999 0.84253064 0.84428996 0.83834604\n",
      " 0.8280068  0.82487201 0.82488631 0.82488631 0.82514459 0.82488631\n",
      " 0.82901999 0.83585906 0.83507541 0.8417098  0.83079495 0.82630227\n",
      " 0.82627955 0.8312966  0.84776284 0.8360193  0.82488631 0.83920391\n",
      " 0.82488631 0.82488631 0.83581309 0.83581309 0.83585906 0.8450845\n",
      " 0.82488631 0.82488631 0.82841043 0.82856943 0.82488631 0.82488631\n",
      " 0.83834604 0.83585906 0.83453927 0.84185881 0.83556602 0.83302929\n",
      " 0.82488631 0.84428996 0.82841043 0.82488631        nan 0.83521469\n",
      " 0.84276655 0.84685032 0.83026191 0.82488631 0.82875603 0.84424201\n",
      " 0.83231997 0.82488631 0.82488631 0.82488631 0.83987605 0.83585906\n",
      " 0.83304336 0.82488631 0.8421527  0.82488631 0.8278815  0.82488631\n",
      " 0.84111541 0.8450615  0.8360193  0.77844887 0.82488631 0.83304336\n",
      " 0.83317591 0.81792929 0.84232656 0.82488631 0.84348217 0.82957319\n",
      " 0.82841043 0.82488631 0.83585906 0.82488631 0.85298087 0.82488631\n",
      " 0.82488631 0.8398029  0.82488631 0.83923165 0.83873321 0.83581309\n",
      " 0.82488631 0.82488631 0.82488631 0.83886618 0.82488631 0.83581309\n",
      " 0.82488631 0.83887417 0.84649426 0.82959544 0.82488631 0.82488631\n",
      " 0.82630227 0.83847851 0.83012345 0.83585906 0.82488631 0.83556602\n",
      " 0.82488631 0.82488631 0.84189061 0.82488631 0.82488631 0.82488631\n",
      " 0.83133873 0.83854665 0.84062648 0.84649426 0.83022876 0.84004255\n",
      " 0.8317569  0.82488631 0.82488631 0.84276783 0.82488631 0.82488631\n",
      " 0.82488631 0.83997506 0.82488631 0.82768187 0.84708677 0.82488631\n",
      " 0.8325115  0.84523013 0.82488631        nan 0.83825509 0.84708677\n",
      "        nan 0.83304336 0.82634772 0.8418381  0.82488631 0.82488631\n",
      " 0.83912379 0.82488631 0.83924658 0.82488631 0.82487201 0.83186859\n",
      " 0.83997506        nan 0.829928   0.82488631 0.8389098  0.84120701\n",
      " 0.82488631 0.82488631 0.82488631 0.82068329 0.83393342 0.8455955\n",
      " 0.77844887 0.82488631 0.82488631 0.83480762 0.82586846 0.82768187\n",
      " 0.82855513 0.82488631 0.84428996 0.84593807 0.83347315 0.83987605\n",
      " 0.82488631 0.82488631 0.83549985 0.82488631 0.84088427 0.83225259\n",
      " 0.84491204 0.82586846 0.85289202 0.82487201 0.82488631        nan\n",
      " 0.8398029  0.82488631 0.84276655 0.82488631 0.82488631 0.82488631\n",
      " 0.83585906 0.82488631 0.82488631 0.83796224 0.84649044 0.84276655\n",
      "        nan 0.8280068  0.82488631 0.83585906 0.83585906 0.82488631\n",
      " 0.82488631 0.83581309 0.82857656 0.83997506 0.83118427 0.82488631\n",
      " 0.82488631 0.83944018 0.83300575 0.83585906 0.82488631 0.83536416\n",
      " 0.82488631 0.82488631 0.82488631        nan 0.82841455 0.82488631\n",
      " 0.83480762 0.82488631 0.82488631 0.83185861 0.82970101 0.82488631\n",
      " 0.84708677 0.83886117 0.83585906 0.82488631 0.77844887 0.83077653\n",
      " 0.82488631 0.82488631 0.82488631 0.83581309 0.77844887 0.84303194\n",
      " 0.82488631 0.82488631 0.82488631 0.83026191 0.82488631 0.82488631\n",
      " 0.8401992  0.83861123 0.84593807 0.82488631 0.82488631 0.83847851\n",
      " 0.83609512 0.84240887 0.83947088 0.82488631 0.8278815  0.83783511\n",
      " 0.82634772 0.82768187 0.82488631 0.83585906 0.82488631 0.84062648\n",
      " 0.83989593 0.82488631 0.82488631 0.82841043 0.83581309 0.82857656\n",
      " 0.8447298  0.77844887 0.82488631 0.82755509 0.82488631 0.85355628\n",
      " 0.77844887 0.82488631 0.83886117 0.83865213 0.82488631 0.84193488\n",
      " 0.82488631 0.82514459 0.83585906 0.82488631 0.82937615 0.83856831\n",
      " 0.82487201 0.83556602 0.77844887 0.83498151 0.83870501 0.8360193\n",
      " 0.84771674 0.83077653 0.82627955 0.8312966  0.82488631 0.82488631\n",
      " 0.82488631 0.82488631 0.82857656 0.82488631 0.82488631 0.82488631\n",
      " 0.82488631 0.83302929 0.82488631 0.82488631 0.84043407 0.82514459\n",
      " 0.8365337  0.82488631 0.83556602 0.83260876 0.83581309 0.82488631\n",
      " 0.82488631 0.82488631 0.8464741  0.82488631 0.82488631 0.82627955\n",
      " 0.84224584 0.83304336 0.8280068  0.85181276 0.82488631 0.82488631\n",
      " 0.82514459 0.82488631 0.84032782 0.82627955 0.84525968 0.82488631\n",
      " 0.82488631 0.83585906 0.8317569  0.77844887 0.83895825 0.82488631\n",
      " 0.77844887 0.82488631 0.84543013 0.83416433 0.84409773 0.84381077\n",
      " 0.83212638 0.82488631 0.84402991 0.84481132 0.82488631 0.82488631\n",
      " 0.82488631 0.77844887 0.82488631        nan 0.84795638 0.77844887\n",
      " 0.82614606 0.82488631 0.84291253 0.84649426 0.82488631 0.82488631\n",
      " 0.82488631 0.84428996 0.84968052 0.8429271  0.82488631 0.83810011\n",
      " 0.77844887 0.82488631 0.82488631 0.83585906 0.83585906 0.83847851\n",
      " 0.8278815  0.83581309 0.82488631 0.83845651 0.84269842 0.82488631\n",
      " 0.84062648 0.84673915 0.77844887 0.83581309 0.82312818 0.84460501\n",
      " 0.82488631 0.82768187 0.83265322 0.82488631 0.82856943 0.82514459\n",
      " 0.82488631 0.82488631 0.82488631 0.83581309 0.82488631 0.83556602\n",
      " 0.82488631        nan 0.84844849        nan 0.82488631 0.82488631\n",
      " 0.82488631 0.82828686 0.83886618        nan 0.82926049 0.84592853\n",
      " 0.8280068  0.82488631 0.84841987 0.83863819 0.83304336 0.83585906\n",
      " 0.82488631 0.82829096 0.77844887 0.84180729 0.83556602 0.83581309\n",
      " 0.84623971 0.84525968 0.77844887 0.82488631 0.82488631 0.84146757\n",
      " 0.82488631 0.82487201 0.82488631 0.82901999 0.84491204 0.83024761\n",
      " 0.82488631 0.8360193  0.82488631 0.84111477 0.82875603 0.82488631\n",
      " 0.82488631 0.83302929]\n",
      "  warnings.warn(\n",
      "C:\\Users\\18137\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.86506357 0.77774902 0.84452029 0.84827272 0.84373893 0.82583601\n",
      " 0.84810619 0.82583601 0.82583601 0.84544502 0.82583601 0.85387731\n",
      " 0.82583601 0.82583601 0.83261379 0.84341642 0.84044549 0.84634707\n",
      " 0.82583601 0.84341642 0.85400728 0.84408099 0.83301112 0.82583601\n",
      " 0.85170765 0.82583601 0.83672211 0.85141261 0.87887658 0.82583601\n",
      " 0.8416385  0.84049152 0.83579519 0.86403712 0.86936089 0.83082791\n",
      " 0.86234046 0.86524418 0.86700072 0.82583601 0.84544502 0.82583601\n",
      " 0.85627133 0.85036837 0.85124742 0.82583601 0.84373893 0.77774902\n",
      " 0.82583601 0.82583601 0.84917181 0.84341642 0.869247   0.86166046\n",
      " 0.83272552 0.86182955 0.83445672 0.84850985 0.84936169 0.84634707\n",
      " 0.83272552 0.82814917 0.82583601 0.82583601 0.82851932 0.82583601\n",
      " 0.83445672 0.84341642 0.86234046 0.86450075 0.83889216 0.82858492\n",
      " 0.83082791 0.83504222 0.86291955 0.84408099 0.82583601 0.86335397\n",
      " 0.82583601 0.82583601 0.84238417 0.84238417 0.84341642 0.85286541\n",
      " 0.82583601 0.82583601 0.83235552 0.82973987 0.82583601 0.82583601\n",
      " 0.84634707 0.84341642 0.85176411 0.85040327 0.8428255  0.83941205\n",
      " 0.82583601 0.84936169 0.83235552 0.82583601        nan 0.8589266\n",
      " 0.84810619 0.86547833 0.83278796 0.82583601 0.84177388 0.85151501\n",
      " 0.8394209  0.82583601 0.82583601 0.82583601 0.84373893 0.84341642\n",
      " 0.84049152 0.82583601 0.85208791 0.82583601 0.83301112 0.82583601\n",
      " 0.8618937  0.8598196  0.84408099 0.77774902 0.82583601 0.84049152\n",
      " 0.83923703 0.96890335 0.87331171 0.82583601 0.86433602 0.84882477\n",
      " 0.83235552 0.82583601 0.84341642 0.82583601 0.87477208 0.82583601\n",
      " 0.82583601 0.86284149 0.82583601 0.85899528 0.86083243 0.84238417\n",
      " 0.82583601 0.82583601 0.82583601 0.85969102 0.82583601 0.84238417\n",
      " 0.82583601 0.84375961 0.85515207 0.85245967 0.82583601 0.82583601\n",
      " 0.82858492 0.86388028 0.83384655 0.84341642 0.82583601 0.8428255\n",
      " 0.82583601 0.82583601 0.84767897 0.82583601 0.82583601 0.82583601\n",
      " 0.83634263 0.86596617 0.84469235 0.85480087 0.8975826  0.84923917\n",
      " 0.83864725 0.82583601 0.82583601 0.85471897 0.82583601 0.82583601\n",
      " 0.82583601 0.84808153 0.82583601 0.83955617 0.85513746 0.82583601\n",
      " 0.83800353 0.85216545 0.82583601        nan 0.84786933 0.85516646\n",
      "        nan 0.84049152 0.84140421 0.84834074 0.82583601 0.82583601\n",
      " 0.86808469 0.82583601 0.86253741 0.82583601 0.82814917 0.86071225\n",
      " 0.84808153        nan 0.86813625 0.82583601 0.84840174 0.8799783\n",
      " 0.82583601 0.82583601 0.82583601 0.86399356 0.85667189 0.86904589\n",
      " 0.77774902 0.82583601 0.82583601 0.84214072 0.83030851 0.83955617\n",
      " 0.83202072 0.82583601 0.84936169 0.85287561 0.86238196 0.84373893\n",
      " 0.82583601 0.82583601 0.85829538 0.82583601 0.86165467 0.83899004\n",
      " 0.85280786 0.83030851 0.88462184 0.82814917 0.82583601        nan\n",
      " 0.86284149 0.82583601 0.84810619 0.82583601 0.82583601 0.82583601\n",
      " 0.84341642 0.82583601 0.82583601 0.86082043 0.85748347 0.84810619\n",
      "        nan 0.83272552 0.82583601 0.84341642 0.84341642 0.82583601\n",
      " 0.82583601 0.84238417 0.84022157 0.84827272 0.8367705  0.82583601\n",
      " 0.82583601 0.86324952 0.86003515 0.84341642 0.82583601 0.85795121\n",
      " 0.82583601 0.82583601 0.82583601        nan 0.83487784 0.82583601\n",
      " 0.84214072 0.82583601 0.82583601 0.93403526 0.83672211 0.82583601\n",
      " 0.85513746 0.86353019 0.84341642 0.82583601 0.77774902 0.83461817\n",
      " 0.82583601 0.82583601 0.82583601 0.84238417 0.77774902 0.8584718\n",
      " 0.82583601 0.82583601 0.82583601 0.83261379 0.82583601 0.82583601\n",
      " 0.88161618 0.84625332 0.85287561 0.82583601 0.82583601 0.86403712\n",
      " 0.84524616 0.86410037 0.86064046 0.82583601 0.83301112 0.85602064\n",
      " 0.84140421 0.83955617 0.82583601 0.84341642 0.82583601 0.84469235\n",
      " 0.84941419 0.82583601 0.82583601 0.83235552 0.84238417 0.84022157\n",
      " 0.86258614 0.77774902 0.82583601 0.862168   0.82583601 0.88287277\n",
      " 0.77774902 0.82583601 0.86353019 0.86268011 0.82583601 0.86488154\n",
      " 0.82583601 0.82851932 0.84341642 0.82583601 0.83455062 0.84370765\n",
      " 0.82811686 0.8428255  0.77774902 0.85834674 0.8496338  0.84408099\n",
      " 0.87175973 0.83461817 0.83082791 0.83504222 0.82583601 0.82583601\n",
      " 0.82583601 0.82583601 0.84022157 0.82583601 0.82583601 0.82583601\n",
      " 0.82583601 0.83954919 0.82583601 0.82583601 0.8525976  0.82848866\n",
      " 0.83996952 0.82583601 0.8428255  0.88267326 0.84238417 0.82583601\n",
      " 0.82583601 0.82583601 0.89460949 0.82583601 0.82583601 0.83082791\n",
      " 0.85416438 0.84049152 0.83272552 0.8751993  0.82583601 0.82583601\n",
      " 0.82848866 0.82583601 0.86687995 0.83082791 0.85360388 0.82583601\n",
      " 0.82583601 0.84341642 0.83864725 0.77774902 0.86278963 0.82583601\n",
      " 0.77774902 0.82583601 0.86407617 0.87963432 0.8558059  0.85691878\n",
      " 0.84882477 0.82583601 0.86416216 0.8636384  0.82583601 0.82583601\n",
      " 0.82583601 0.77774902 0.82583601        nan 0.86344941 0.77774902\n",
      " 0.82855599 0.82583601 0.85420662 0.85515207 0.82583601 0.82583601\n",
      " 0.82583601 0.84936169 0.86599713 0.85378376 0.82583601 0.92446507\n",
      " 0.77774902 0.82583601 0.82583601 0.84341642 0.84341642 0.86388028\n",
      " 0.83301112 0.84238417 0.82583601 0.84714625 0.86224713 0.82583601\n",
      " 0.84469235 0.85701515 0.77774902 0.84238417 0.8717917  0.85761337\n",
      " 0.82583601 0.83940381 0.88032909 0.82583601 0.82957533 0.82851932\n",
      " 0.82583601 0.82583601 0.82583601 0.84238417 0.82583601 0.8428255\n",
      " 0.82583601        nan 0.86343883        nan 0.82583601 0.82583601\n",
      " 0.82583601 0.8650356  0.85969102        nan 0.83397202 0.85570267\n",
      " 0.83272552 0.82583601 0.87643333 0.85634039 0.84049152 0.84341642\n",
      " 0.82583601 0.84935738 0.77774902 0.87027883 0.8428255  0.84238417\n",
      " 0.85271306 0.85360388 0.77774902 0.82583601 0.82583601 0.85501294\n",
      " 0.82583601 0.82814917 0.82583601 0.83442467 0.85280786 0.83524992\n",
      " 0.82583601 0.84408099 0.82583601 0.85117133 0.84193843 0.82583601\n",
      " 0.82583601 0.83941205]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,60),  \n",
    "    'min_samples_leaf': np.arange(1,60),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00658fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8481724 Precision=0.8568665 Recall=0.8342750 F1=0.8454198\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e9620",
   "metadata": {},
   "source": [
    "# GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e29b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4200 candidates, totalling 21000 fits\n",
      "The best precision score is 0.8605626315693831\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 10, 'max_leaf_nodes': 70, 'min_impurity_decrease': 0.0034, 'min_samples_leaf': 4, 'min_samples_split': 15}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(15,20),  \n",
    "    'min_samples_leaf': np.arange(1,5),\n",
    "    'min_impurity_decrease': np.arange(0.0034, 0.0040, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(70,76), \n",
    "    'max_depth': np.arange(10,15), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07aca40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8481724 Precision=0.8568665 Recall=0.8342750 F1=0.8454198\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b8af8",
   "metadata": {},
   "source": [
    "In the Confusion matrix results above, we can see that we have the precision score to be the highest \n",
    "compared to the rest of the performance metrics followed by Accuracy and F1 respectively.\n",
    "\n",
    "Precision=0.8568665\n",
    "Accuracy=0.8481724\n",
    "F1=0.8454198\n",
    "Recall=0.8342750 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302bd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
